{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e8f07f-7bc0-4187-9bf6-033cf502f9ab",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9861c4-b795-46ec-a591-cbf432b0a22a",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89f019-d3ee-459e-99f0-a78f2ed7d22d",
   "metadata": {},
   "source": [
    "### Import nessesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ebac478c-bce3-404d-a010-1e7bc976fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d84f5d7f-609b-4e88-a3da-73d88c908685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2fae8b-b509-4466-8103-4771244a9a45",
   "metadata": {},
   "source": [
    "### Hyper Parameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "de8ba377-9189-46f3-8761-75e10681b2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    image_size = 128                                                         # Size of images (128x128 or 256x256)\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    lr = 0.00005                                                              # Learning rate (standard for GANs)\n",
    "    beta1 = 0.5                                                              # Beta1 hyperparam for Adam optimizers\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data_path = \"./textile_dataset/\"                                         # FOLDER WHERE YOU PUT YOUR IMAGES\n",
    "    output_path = \"./generated_patterns\"\n",
    "\n",
    "os.makedirs(Config.output_path, exist_ok=True)\n",
    "os.makedirs(Config.data_path, exist_ok=True)\n",
    "\n",
    "print(f\"Running on device: {Config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d202a-0689-4b1d-b10c-ef067c431ceb",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ee773-3f2a-4d56-8bed-ac71f0eb8ccf",
   "metadata": {},
   "source": [
    "### Data Loader Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "83701a29-f07f-4a3e-8100-d759b54b7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextileDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        if len(self.image_files) == 0:\n",
    "            print(f\"WARNING: No images found in {root_dir}. Please add carpet/cloth images.\")\n",
    "            # We create dummy data if folder is empty so code doesn't crash\n",
    "            self.dummy = True\n",
    "        else:\n",
    "            self.dummy = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files) if self.image_files else 100 # Return 100 if dummy\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.dummy:\n",
    "            # Create a synthetic texture (noise) if no data provided\n",
    "            return torch.randn(3, Config.image_size, Config.image_size)\n",
    "            \n",
    "        img_name = os.path.join(self.root_dir, self.image_files[idx % len(self.image_files)])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf137d9-1c8d-4e3d-bcc3-64982e8dd7a8",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "8103c6d7-5d03-4755-8a03-1d44e4a138dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((Config.image_size, Config.image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b613a2-22df-4e31-9333-45922a9985e6",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "19761081-50ff-4fb9-a4d7-0ac1ca3457fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextileDataset(Config.data_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=Config.batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "36d67e63-5683-4eac-84ce-a0e9b2c8dee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2068"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d55824-d81e-4531-a012-0945f50c8889",
   "metadata": {},
   "source": [
    "# Fractal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b012e6d-06d5-4db1-b5e8-2be045894ad8",
   "metadata": {},
   "source": [
    "## Fractal Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7468af13-43fc-479c-ad01-6116a4c59e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalEngine:\n",
    "    \"\"\"\n",
    "    Generates mathematical fractals to serve as the structural condition\n",
    "    for the generative model.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def generate_julia(width, height, c_real, c_imag, zoom=1.0):\n",
    "        # Create a grid of complex numbers\n",
    "        x = np.linspace(-1.5/zoom, 1.5/zoom, width)\n",
    "        y = np.linspace(-1.5/zoom, 1.5/zoom, height)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = X + 1j * Y\n",
    "        C = complex(c_real, c_imag)\n",
    "        \n",
    "        img = np.zeros(Z.shape, dtype=float)\n",
    "        mask = np.ones(Z.shape, dtype=bool)\n",
    "        \n",
    "        # Iteration count (Simulating fractal depth)\n",
    "        max_iter = 30\n",
    "        for i in range(max_iter):\n",
    "            Z[mask] = Z[mask] * Z[mask] + C\n",
    "            mask = (np.abs(Z) < 10)\n",
    "            img[mask] = i\n",
    "            \n",
    "        # Normalize to 0-1\n",
    "        img = img / max_iter\n",
    "        return torch.tensor(img, dtype=torch.float32).unsqueeze(0) # (1, H, W)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_random_batch(batch_size, size):\n",
    "        \"\"\"Generates a batch of random fractals\"\"\"\n",
    "        fractals = []\n",
    "        for _ in range(batch_size):\n",
    "            # Randomize fractal parameters\n",
    "            c_re = random.uniform(-0.8, 0.0)\n",
    "            c_im = random.uniform(0.0, 0.7)\n",
    "            zoom = random.uniform(0.8, 1.2)\n",
    "            \n",
    "            f = FractalEngine.generate_julia(size, size, c_re, c_im, zoom)\n",
    "            fractals.append(f)\n",
    "        return torch.stack(fractals).to(Config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f6440-0350-4447-bd3d-8dd80a39b871",
   "metadata": {},
   "source": [
    "# Main Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f2bd07-3e1a-479a-b301-45961b9c34e8",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05768e1-94d9-413b-bb0d-d5c3326d057c",
   "metadata": {},
   "source": [
    "(Pix2Pix Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56040297-f18b-447e-b787-cb75267eadf5",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d700b-ea22-41c7-986d-9f7d5980b326",
   "metadata": {},
   "source": [
    "Generator: U-Net (Encoder-Decoder with Skip Connections)\n",
    "\n",
    "Input: 1-channel Fractal -> Output: 3-channel Textile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2dad614d-80ce-4536-b845-d21b9557a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "        \n",
    "        def down_block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False)]\n",
    "            if normalize: layers.append(nn.BatchNorm2d(out_feat))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        def up_block(in_feat, out_feat, dropout=0.0):\n",
    "            layers = [\n",
    "                nn.ConvTranspose2d(in_feat, out_feat, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_feat),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            if dropout: layers.append(nn.Dropout(dropout))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        # Encoder\n",
    "        self.down1 = down_block(1, 64, normalize=False) # 128 -> 64\n",
    "        self.down2 = down_block(64, 128)               # 64 -> 32\n",
    "        self.down3 = down_block(128, 256)              # 32 -> 16\n",
    "        self.down4 = down_block(256, 512)              # 16 -> 8\n",
    "        self.down5 = down_block(512, 512)              # 8 -> 4\n",
    "        \n",
    "        # Decoder (with Skip Connections inputs calculated in forward)\n",
    "        self.up1 = up_block(512, 512, dropout=0.5)     # 4 -> 8\n",
    "        self.up2 = up_block(1024, 256, dropout=0.5)    # 8 -> 16 (input is cat(up1, down4))\n",
    "        self.up3 = up_block(512, 128)                  # 16 -> 32\n",
    "        self.up4 = up_block(256, 64)                   # 32 -> 64\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 3, 4, 2, 1),       # 64 -> 128\n",
    "            nn.Tanh() # Output -1 to 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Down\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        \n",
    "        # Up (Concatenate with skip connections)\n",
    "        u1 = self.up1(d5)\n",
    "        u2 = self.up2(torch.cat([u1, d4], 1))\n",
    "        u3 = self.up3(torch.cat([u2, d3], 1))\n",
    "        u4 = self.up4(torch.cat([u3, d2], 1))\n",
    "        \n",
    "        out = self.final(torch.cat([u4, d1], 1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7dad18-a835-4589-816f-febfab17374c",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17811cd-1430-467d-b238-5047b16ddc1a",
   "metadata": {},
   "source": [
    "PatchGAN\n",
    "\n",
    "Input: 3-channel Image -> Output: 1-channel Real/Fake Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f00c31a3-dd90-4c80-a435-685cba308bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, 2, 1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            discriminator_block(3, 64, normalization=False),\n",
    "            discriminator_block(64, 128),\n",
    "            discriminator_block(128, 256),\n",
    "            discriminator_block(256, 512),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94edb7-8721-41f7-b18a-b580862e740c",
   "metadata": {},
   "source": [
    "# Trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa2687f-bd91-4765-8018-a3953bb6d838",
   "metadata": {},
   "source": [
    "## Innitializing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0162519-6d68-4656-9ed4-949afccfa377",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "94d82263-4f1a-4924-9fce-18ea237f790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = UNetGenerator().to(Config.device)\n",
    "discriminator = PatchDiscriminator().to(Config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863daba-35a1-4801-a9b5-c1a1df47761e",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30925048-54e5-41ca-bde2-0f0318881439",
   "metadata": {},
   "source": [
    "> Standard GAN Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e8fcd02a-5731-4cc9-8109-82d1f45e4708",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_GAN = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f40bf67-1a7b-4fd6-b3bb-421ca5fd9f63",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3b128996-235b-47d7-badd-b861c8a3190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = optim.Adam(generator.parameters(), lr=Config.lr, betas=(Config.beta1, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=Config.lr, betas=(Config.beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2103f4a2-1765-402e-be0d-a35a665816af",
   "metadata": {},
   "source": [
    "### Weights init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "ad210f40-1167-434e-b772-724345a5c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "c7b0d53b-99a0-4077-8b5d-e61f1b74a394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatchDiscriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (4): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.apply(weights_init)\n",
    "discriminator.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f527092-9e73-49b7-adcb-b29f8ae9ded5",
   "metadata": {},
   "source": [
    "## Tranning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d62bb4b6-d4cb-4534-b6db-4ccd30045ac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]: 100%|██| 65/65 [02:28<00:00,  2.29s/it, D_loss=0.368, G_loss=8.52]\n",
      "Epoch [2/50]: 100%|██| 65/65 [02:36<00:00,  2.41s/it, D_loss=0.301, G_loss=8.06]\n",
      "Epoch [3/50]: 100%|████| 65/65 [02:35<00:00,  2.39s/it, D_loss=0.4, G_loss=8.42]\n",
      "Epoch [4/50]: 100%|███| 65/65 [02:38<00:00,  2.44s/it, D_loss=0.708, G_loss=8.3]\n",
      "Epoch [5/50]: 100%|██| 65/65 [02:28<00:00,  2.28s/it, D_loss=0.276, G_loss=7.55]\n",
      "Epoch [6/50]: 100%|████| 65/65 [02:17<00:00,  2.12s/it, D_loss=0.5, G_loss=7.12]\n",
      "Epoch [7/50]: 100%|██| 65/65 [02:09<00:00,  1.99s/it, D_loss=0.551, G_loss=7.59]\n",
      "Epoch [8/50]: 100%|██| 65/65 [02:13<00:00,  2.06s/it, D_loss=0.541, G_loss=8.38]\n",
      "Epoch [9/50]: 100%|██| 65/65 [02:16<00:00,  2.09s/it, D_loss=0.659, G_loss=8.07]\n",
      "Epoch [10/50]: 100%|█| 65/65 [02:17<00:00,  2.11s/it, D_loss=0.423, G_loss=8.74]\n",
      "Epoch [11/50]: 100%|███| 65/65 [02:19<00:00,  2.14s/it, D_loss=0.4, G_loss=8.14]\n",
      "Epoch [12/50]: 100%|█| 65/65 [02:28<00:00,  2.29s/it, D_loss=0.513, G_loss=7.87]\n",
      "Epoch [13/50]: 100%|██| 65/65 [02:24<00:00,  2.23s/it, D_loss=0.55, G_loss=7.71]\n",
      "Epoch [14/50]: 100%|█| 65/65 [02:23<00:00,  2.21s/it, D_loss=0.509, G_loss=7.73]\n",
      "Epoch [15/50]: 100%|███| 65/65 [02:14<00:00,  2.07s/it, D_loss=0.5, G_loss=7.57]\n",
      "Epoch [16/50]: 100%|█| 65/65 [02:20<00:00,  2.17s/it, D_loss=0.546, G_loss=8.14]\n",
      "Epoch [17/50]: 100%|█| 65/65 [02:22<00:00,  2.19s/it, D_loss=0.545, G_loss=8.24]\n",
      "Epoch [18/50]: 100%|█| 65/65 [02:19<00:00,  2.15s/it, D_loss=0.576, G_loss=8.16]\n",
      "Epoch [19/50]: 100%|██| 65/65 [02:13<00:00,  2.05s/it, D_loss=0.677, G_loss=8.2]\n",
      "Epoch [20/50]: 100%|█| 65/65 [02:13<00:00,  2.05s/it, D_loss=0.457, G_loss=8.34]\n",
      "Epoch [21/50]: 100%|█| 65/65 [02:12<00:00,  2.04s/it, D_loss=0.651, G_loss=7.74]\n",
      "Epoch [22/50]: 100%|█| 65/65 [02:13<00:00,  2.05s/it, D_loss=0.461, G_loss=7.81]\n",
      "Epoch [23/50]: 100%|█| 65/65 [02:14<00:00,  2.07s/it, D_loss=0.405, G_loss=8.59]\n",
      "Epoch [24/50]: 100%|██| 65/65 [02:11<00:00,  2.03s/it, D_loss=0.495, G_loss=7.4]\n",
      "Epoch [25/50]: 100%|█| 65/65 [02:10<00:00,  2.01s/it, D_loss=0.419, G_loss=8.07]\n",
      "Epoch [26/50]: 100%|█| 65/65 [02:41<00:00,  2.49s/it, D_loss=0.684, G_loss=7.96]\n",
      "Epoch [27/50]: 100%|█| 65/65 [02:36<00:00,  2.41s/it, D_loss=0.379, G_loss=7.87]\n",
      "Epoch [28/50]: 100%|█| 65/65 [02:09<00:00,  1.99s/it, D_loss=0.672, G_loss=8.65]\n",
      "Epoch [29/50]: 100%|█| 65/65 [02:09<00:00,  2.00s/it, D_loss=0.477, G_loss=7.66]\n",
      "Epoch [30/50]: 100%|█| 65/65 [02:58<00:00,  2.74s/it, D_loss=0.466, G_loss=8.13]\n",
      "Epoch [31/50]: 100%|█| 65/65 [02:27<00:00,  2.28s/it, D_loss=0.329, G_loss=7.35]\n",
      "Epoch [32/50]: 100%|█| 65/65 [02:09<00:00,  2.00s/it, D_loss=0.326, G_loss=8.35]\n",
      "Epoch [33/50]: 100%|█| 65/65 [02:15<00:00,  2.09s/it, D_loss=0.625, G_loss=7.28]\n",
      "Epoch [34/50]: 100%|█| 65/65 [02:17<00:00,  2.11s/it, D_loss=0.559, G_loss=7.87]\n",
      "Epoch [35/50]: 100%|█| 65/65 [02:19<00:00,  2.14s/it, D_loss=0.494, G_loss=8.39]\n",
      "Epoch [36/50]: 100%|█| 65/65 [02:21<00:00,  2.18s/it, D_loss=0.604, G_loss=8.24]\n",
      "Epoch [37/50]: 100%|█| 65/65 [02:27<00:00,  2.27s/it, D_loss=0.509, G_loss=7.56]\n",
      "Epoch [38/50]: 100%|█| 65/65 [02:37<00:00,  2.43s/it, D_loss=0.449, G_loss=7.75]\n",
      "Epoch [39/50]: 100%|█| 65/65 [02:23<00:00,  2.21s/it, D_loss=0.576, G_loss=8.09]\n",
      "Epoch [40/50]: 100%|█| 65/65 [02:15<00:00,  2.08s/it, D_loss=0.461, G_loss=8.48]\n",
      "Epoch [41/50]: 100%|█| 65/65 [02:12<00:00,  2.03s/it, D_loss=0.345, G_loss=8.38]\n",
      "Epoch [42/50]: 100%|█| 65/65 [02:11<00:00,  2.03s/it, D_loss=0.556, G_loss=7.49]\n",
      "Epoch [43/50]: 100%|█| 65/65 [02:10<00:00,  2.01s/it, D_loss=0.607, G_loss=7.72]\n",
      "Epoch [44/50]: 100%|█| 65/65 [02:09<00:00,  2.00s/it, D_loss=0.472, G_loss=8.69]\n",
      "Epoch [45/50]: 100%|█| 65/65 [02:09<00:00,  2.00s/it, D_loss=0.463, G_loss=8.16]\n",
      "Epoch [46/50]: 100%|█| 65/65 [02:09<00:00,  2.00s/it, D_loss=0.488, G_loss=7.61]\n",
      "Epoch [47/50]: 100%|█| 65/65 [02:13<00:00,  2.06s/it, D_loss=0.473, G_loss=8.13]\n",
      "Epoch [48/50]: 100%|█| 65/65 [02:12<00:00,  2.03s/it, D_loss=0.492, G_loss=8.29]\n",
      "Epoch [49/50]: 100%|███| 65/65 [02:10<00:00,  2.01s/it, D_loss=0.49, G_loss=8.5]\n",
      "Epoch [50/50]: 100%|█| 65/65 [02:24<00:00,  2.22s/it, D_loss=0.604, G_loss=7.41]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "for epoch in range(1, Config.epochs + 1):\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    \n",
    "    for i, real_imgs in enumerate(loop):\n",
    "        batch_curr_size = real_imgs.size(0)\n",
    "        real_imgs = real_imgs.to(Config.device)\n",
    "        \n",
    "        # 1. Generate Fractal Conditions\n",
    "        fractal_condition = FractalEngine.get_random_batch(batch_curr_size, Config.image_size)\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Real Images\n",
    "        # Note: In standard Pix2Pix we pair inputs. Here we use Unpaired training \n",
    "        # (similar to Texture Synthesis). We want Real to be classified as Real.\n",
    "        label_real = torch.ones(batch_curr_size, 1, 4, 4).to(Config.device) * 0.9 # Label smoothing\n",
    "        output_real = discriminator(real_imgs)\n",
    "        # PatchGAN outputs a grid, we resize label to match\n",
    "        label_real = torch.ones_like(output_real).to(Config.device)\n",
    "        loss_D_real = criterion_GAN(output_real, label_real)\n",
    "        \n",
    "        # Fake Images\n",
    "        fake_imgs = generator(fractal_condition)\n",
    "        label_fake = torch.zeros_like(output_real).to(Config.device)\n",
    "        output_fake = discriminator(fake_imgs.detach()) # Detach to avoid G gradients\n",
    "        loss_D_fake = criterion_GAN(output_fake, label_fake)\n",
    "        \n",
    "        loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Generator wants Discriminator to think images are Real\n",
    "        output_fake_for_G = discriminator(fake_imgs)\n",
    "        label_real_for_G = torch.ones_like(output_fake_for_G).to(Config.device)\n",
    "        \n",
    "        # Structural Loss (Optional but good): \n",
    "        # Enforce that the output intensity roughly matches fractal intensity \n",
    "        # to preserve the pattern shape.\n",
    "        fake_gray = torch.mean(fake_imgs, dim=1, keepdim=True)\n",
    "        # Normalize fractal to -1 to 1 for comparison\n",
    "        fractal_norm = (fractal_condition - 0.5) / 0.5 \n",
    "        loss_structure = torch.mean(torch.abs(fake_gray - fractal_norm)) * 10 \n",
    "        \n",
    "        loss_G_GAN = criterion_GAN(output_fake_for_G, label_real_for_G)\n",
    "        loss_G = loss_G_GAN + loss_structure\n",
    "        \n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Update progress bar\n",
    "        loop.set_description(f\"Epoch [{epoch}/{Config.epochs}]\")\n",
    "        loop.set_postfix(D_loss=loss_D.item(), G_loss=loss_G.item())\n",
    "        \n",
    "    # Save Generated Images every few epochs\n",
    "    if epoch % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            # Generate a consistent visualization\n",
    "            # 1. Fractal Input (Grayscale)\n",
    "            # 2. Generated Textile (RGB)\n",
    "            viz_fractal = fractal_condition[:4].repeat(1,3,1,1) # Make 3 channel for stacking\n",
    "            viz_gen = fake_imgs[:4]\n",
    "            \n",
    "            # Denormalize for saving\n",
    "            viz_gen = (viz_gen * 0.5) + 0.5\n",
    "            \n",
    "            # Stack vertically: Top row = Fractals, Bottom row = Patterns\n",
    "            grid = torch.cat([viz_fractal, viz_gen], dim=0)\n",
    "            vutils.save_image(grid, f\"{Config.output_path}/epoch_{epoch}.png\", nrow=4)\n",
    "            \n",
    "            # Save Model\n",
    "            torch.save(generator.state_dict(), f\"{Config.output_path}/generator.pth\")\n",
    "\n",
    "print(\"Training Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c3c7a-fa69-4ca2-a0f2-a62e249aadc4",
   "metadata": {},
   "source": [
    "# Infrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca762786-8262-4f02-aa52-f2cfb44fd90e",
   "metadata": {},
   "source": [
    "## Input and Output definning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "dde16be3-5322-4e45-9233-68e1be9d8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_design(save_name=\"final_design.png\"):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        # User defines a specific fractal parameter for their art\n",
    "        # e.g., A nice Julia set\n",
    "        f = FractalEngine.generate_julia(Config.image_size, Config.image_size, -0.7, 0.27015, zoom=1.0)\n",
    "        f = f.unsqueeze(0).to(Config.device)\n",
    "        \n",
    "        out = generator(f)\n",
    "        out = (out * 0.5) + 0.5 # Denormalize\n",
    "        \n",
    "        vutils.save_image(out, save_name)\n",
    "        print(f\"Design saved to {save_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c18bb5-0edb-4421-8f4b-074500ebaab2",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "48db632f-1411-4f3a-98c1-4baca3f39122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design saved to final_design.png\n"
     ]
    }
   ],
   "source": [
    "generate_design()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
